{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_files_tls = [\"1/tlsattributes.csv\",   \n",
    "#             \"2/tlsattributes.csv\",\n",
    "#             \"3/tlsattributes.csv\",\n",
    "#             \"4/tlsattributes.csv\",\n",
    "#             \"5/tlsattributes.csv\",\n",
    "#             \"6/tlsattributes.csv\",\n",
    "#             \"7/tlsattributes.csv\",\n",
    "#             \"8/tlsattributes.csv\",\n",
    "#             \"9/tlsattributes.csv\",\n",
    "#             \"10/tlsattributes.csv\",\n",
    "#             \"11/tlsattributes.csv\",\n",
    "#             \"12/tlsattributes.csv\"\n",
    "#             ]\n",
    "# outfile = 'tlsattributes.csv'\n",
    "\n",
    "# all_files_tls = [\"1/httpattributes.csv\",   \n",
    "#             \"2/httpattributes.csv\",\n",
    "#             \"3/httpattributes.csv\",\n",
    "#             \"4/httpattributes.csv\",\n",
    "#             \"5/httpattributes.csv\",\n",
    "#             \"6/httpattributes.csv\",\n",
    "#             \"7/httpattributes.csv\",\n",
    "#             \"8/httpattributes.csv\",\n",
    "#             \"9/httpattributes.csv\",\n",
    "#             \"10/httpattributes.csv\",\n",
    "#             \"11/httpattributes.csv\",\n",
    "#             \"12/httpattributes.csv\"\n",
    "#             ]\n",
    "# outfile = 'httpattributes.csv'\n",
    "\n",
    "# all_files_tls = [\"1/ntpattributes.csv\",   \n",
    "#             \"2/ntpattributes.csv\",\n",
    "#             \"3/ntpattributes.csv\",\n",
    "#             \"4/ntpattributes.csv\",\n",
    "#             \"5/ntpattributes.csv\",\n",
    "#             \"6/ntpattributes.csv\",\n",
    "#             \"7/ntpattributes.csv\",\n",
    "#             \"8/ntpattributes.csv\",\n",
    "#             \"9/ntpattributes.csv\",\n",
    "#             \"10/ntpattributes.csv\",\n",
    "#             \"11/ntpattributes.csv\",\n",
    "#             \"12/ntpattributes.csv\"\n",
    "#             ]\n",
    "# outfile = 'ntpattributes.csv'\n",
    "\n",
    "# all_files_tls = [\"1/dhcpattributes.csv\",   \n",
    "#             \"2/dhcpattributes.csv\",\n",
    "#             \"3/dhcpattributes.csv\",\n",
    "#             \"4/dhcpattributes.csv\",\n",
    "#             \"5/dhcpattributes.csv\",\n",
    "#             \"6/dhcpattributes.csv\",\n",
    "#             \"7/dhcpattributes.csv\",\n",
    "#             \"8/dhcpattributes.csv\",\n",
    "#             \"9/dhcpattributes.csv\",\n",
    "#             \"10/dhcpattributes.csv\",\n",
    "#             \"11/dhcpattributes.csv\",\n",
    "#             \"12/dhcpattributes.csv\"\n",
    "#             ]\n",
    "# outfile = 'dhcpattributes.csv'\n",
    "\n",
    "# all_files_tls = [\"1/ssdpattributes.csv\",   \n",
    "#             \"2/ssdpattributes.csv\",\n",
    "#             \"3/ssdpattributes.csv\",\n",
    "#             \"4/ssdpattributes.csv\",\n",
    "#             \"5/ssdpattributes.csv\",\n",
    "#             \"6/ssdpattributes.csv\",\n",
    "#             \"7/ssdpattributes.csv\",\n",
    "#             \"8/ssdpattributes.csv\",\n",
    "#             \"9/ssdpattributes.csv\",\n",
    "#             \"10/ssdpattributes.csv\",\n",
    "#             \"11/ssdpattributes.csv\",\n",
    "#             \"12/ssdpattributes.csv\"\n",
    "#             ]\n",
    "# outfile = 'ssdpattributes.csv'\n",
    "\n",
    "all_files_tls = [\"1/dnsattributes.csv\",   \n",
    "            \"2/dnsattributes.csv\",\n",
    "            \"3/dnsattributes.csv\",\n",
    "            \"4/dnsattributes.csv\",\n",
    "            \"5/dnsattributes.csv\",\n",
    "            \"6/dnsattributes.csv\",\n",
    "            \"7/dnsattributes.csv\",\n",
    "            \"8/dnsattributes.csv\",\n",
    "            \"9/dnsattributes.csv\",\n",
    "            \"10/dnsattributes.csv\",\n",
    "            \"11/dnsattributes.csv\",\n",
    "            \"12/dnsattributes.csv\"\n",
    "            ]\n",
    "outfile = 'dnsattributes.csv'\n",
    "\n",
    "# data/flowResults/V5/AmazonEcho/1/AmazonEcho_44650d56ccd3_flowResult.csv\n",
    "# data/flowResults/V5/AmazonEcho/concatAmazonEcho.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: 1/dnsattributes.csv exists\n",
      "filename: 2/dnsattributes.csv exists\n",
      "filename: 3/dnsattributes.csv exists\n",
      "filename: 4/dnsattributes.csv exists\n",
      "filename: 5/dnsattributes.csv exists\n",
      "filename: 6/dnsattributes.csv exists\n",
      "filename: 7/dnsattributes.csv exists\n",
      "filename: 8/dnsattributes.csv exists\n",
      "filename: 9/dnsattributes.csv exists\n",
      "filename: 10/dnsattributes.csv exists\n",
      "filename: 11/dnsattributes.csv exists\n",
      "filename: 12/dnsattributes.csv exists\n",
      "   srcMac   dstMac  ethType          srcIp          dstIp   ipProto   srcPort  \\\n",
      "0     NaN      NaN   0x0800  192.168.1.248    192.168.1.1        17     57927   \n",
      "1     NaN      NaN   0x0800  192.168.1.248    192.168.1.1        17     43188   \n",
      "2     NaN      NaN   0x0800    192.168.1.1  192.168.1.248        17        53   \n",
      "3     NaN      NaN   0x0800    192.168.1.1  192.168.1.248        17        53   \n",
      "4     NaN      NaN   0x0800    192.168.1.1  192.168.1.248        17        53   \n",
      "\n",
      "    dstPort transaction id  flags  Qnumber  \\\n",
      "0        53           d168    100        1   \n",
      "1        53           b6c0    100        1   \n",
      "2     34619           880e    100        1   \n",
      "3     43692           2494    100        1   \n",
      "4     44660           bb97    100        1   \n",
      "\n",
      "                                domain name,,  \n",
      "0  04786d70700f73616d73756e67736d6172746361,,  \n",
      "1  04786d70700f73616d73756e67736d6172746361,,  \n",
      "2  04786d70700f73616d73756e67736d6172746361,,  \n",
      "3  04786d70700f73616d73756e67736d6172746361,,  \n",
      "4  04706f6f6c036e7470036f72670000010001db8e,,  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13070/1589227019.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for filename in all_files_tls:\n",
    "    if Path(filename).exists():\n",
    "        print(f'filename: {filename} exists')\n",
    "        df = pd.read_csv(filename, sep=',,,')\n",
    "        all_dfs.append(df)\n",
    "    else:\n",
    "        print(f'filename: {filename} does not exist')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the nummber of flows in each sub csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6479\n",
      "3456\n",
      "1645\n",
      "87\n",
      "149\n",
      "8858\n",
      "157\n",
      "41\n",
      "153\n",
      "106\n",
      "38\n",
      "69\n",
      "Total = 21238\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for df in all_dfs:\n",
    "    print(len(df))\n",
    "    total += len(df)\n",
    "print(\"Total =\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing all the dfs to the same csv file\n",
    "frame = pd.concat(all_dfs, axis=0)\n",
    "# print(frame.info())\n",
    "# print(len(frame.groupby([' srcIp', ' dstIp', ' ipProto', ' srcPort', ' dstPort'])[' protocol'].count()))\n",
    "# print(frame.columns)\n",
    "\n",
    "# writing the concatanated df and reading the same again --> done to overcome the unknown error of duplicate rows not being dropped if this is not done\n",
    "frame.to_csv(outfile, index=False, na_rep='null', sep='|')\n",
    "# frame = pd.read_csv('DUP_CONCATANATED_AmazonEcho_44650d56ccd3_flowResult.csv')\n",
    "\n",
    "\n",
    "\n",
    "# # dropping the duplicate flows\n",
    "# result_frame = frame.drop_duplicates()\n",
    "# result_frame_1 = frame.drop_duplicates(keep=False)\n",
    "\n",
    "# # writing the resulting frame to a csv file\n",
    "# result_frame.to_csv('01DUP_CONCATANATED_AmazonEcho_44650d56ccd3_flowResult.csv', index=False, na_rep='null')\n",
    "# result_frame_1.to_csv('NODUP_AmazonEcho_44650d56ccd3_flowResult.csv.csv', index=False, na_rep='null')\n",
    "\n",
    "# print(\"lengths of before dropping duplicates:\", len(frame))\n",
    "# print(\"number of duplicate entries dropped:\", len(frame) - len(result_frame))\n",
    "# print(\"number of entries with duplicate entries:\", len(result_frame) - len(result_frame_1))\n",
    "# print(\"number of entries without duplicate entries:\", len(result_frame_1))\n",
    "# print(\"number of entries in result_frame:\", len(result_frame), end=\"\")\n",
    "# print(\"    (equal to the addition of the above 2)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ip_unique = result_frame.groupby([' srcIp', ' dstIp', ' ipProto', ' srcPort', ' dstPort']).count()  # [' protocol]'\n",
    "# print(df_ip_unique)\n",
    "# # df_1 = df.loc[df[' ipProto'].str.contains('6') & (df[' dstPort']=='8883')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
