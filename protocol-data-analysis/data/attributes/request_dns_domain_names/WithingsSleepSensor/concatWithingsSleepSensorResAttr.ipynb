{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_files_tls = [\"1/tlsattributes.csv\",   \n",
    "#             \"2/tlsattributes.csv\",\n",
    "#             \"3/tlsattributes.csv\",\n",
    "#             \"4/tlsattributes.csv\",\n",
    "#             \"5/tlsattributes.csv\",\n",
    "#             \"6/tlsattributes.csv\",\n",
    "#             \"7/tlsattributes.csv\",\n",
    "#             \"8/tlsattributes.csv\",\n",
    "#             \"9/tlsattributes.csv\",\n",
    "#             \"10/tlsattributes.csv\",\n",
    "#             \"11/tlsattributes.csv\",\n",
    "#             \"12/tlsattributes.csv\"\n",
    "#             ]\n",
    "\n",
    "# all_files_tls = [\"1/httpattributes.csv\",   \n",
    "#             \"2/httpattributes.csv\",\n",
    "#             \"3/httpattributes.csv\",\n",
    "#             \"4/httpattributes.csv\"\n",
    "#             ]\n",
    "# outfile = 'httpattributes.csv'\n",
    "\n",
    "# all_files_tls = [\"1/ntpattributes.csv\",   \n",
    "#             \"2/ntpattributes.csv\",\n",
    "#             \"3/ntpattributes.csv\",\n",
    "#             \"4/ntpattributes.csv\"\n",
    "#             ]\n",
    "# outfile = 'ntpattributes.csv'\n",
    "\n",
    "# all_files_tls = [\"1/dhcpattributes.csv\",   \n",
    "#             \"2/dhcpattributes.csv\",\n",
    "#             \"3/dhcpattributes.csv\",\n",
    "#             \"4/dhcpattributes.csv\"\n",
    "#             ]\n",
    "# outfile = 'dhcpattributes.csv'\n",
    "\n",
    "# all_files_tls = [\"1/ssdpattributes.csv\",   \n",
    "#             \"2/ssdpattributes.csv\",\n",
    "#             \"3/ssdpattributes.csv\",\n",
    "#             \"4/ssdpattributes.csv\"\n",
    "#             ]\n",
    "# outfile = 'ssdpattributes.csv'\n",
    "\n",
    "all_files_tls = [\"1/dnsattributes.csv\",   \n",
    "            \"2/dnsattributes.csv\",\n",
    "            \"3/dnsattributes.csv\",\n",
    "            \"4/dnsattributes.csv\"\n",
    "            ]\n",
    "outfile = 'dnsattributes.csv'\n",
    "# data/flowResults/V5/AmazonEcho/1/AmazonEcho_44650d56ccd3_flowResult.csv\n",
    "# data/flowResults/V5/AmazonEcho/concatAmazonEcho.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: 1/dnsattributes.csv exists\n",
      "filename: 2/dnsattributes.csv exists\n",
      "filename: 3/dnsattributes.csv exists\n",
      "filename: 4/dnsattributes.csv exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13098/3095161664.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13098/3095161664.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13098/3095161664.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n",
      "/tmp/ipykernel_13098/3095161664.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=',,,')\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for filename in all_files_tls:\n",
    "    if Path(filename).exists():\n",
    "        print(f'filename: {filename} exists')\n",
    "        df = pd.read_csv(filename, sep=',,,')\n",
    "        all_dfs.append(df)\n",
    "    else:\n",
    "        print(f'filename: {filename} does not exist')\n",
    "\n",
    "# print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the nummber of flows in each sub csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4560\n",
      "4518\n",
      "5173\n",
      "4510\n",
      "Total = 18761\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for df in all_dfs:\n",
    "    print(len(df))\n",
    "    total += len(df)\n",
    "print(\"Total =\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing all the dfs to the same csv file\n",
    "frame = pd.concat(all_dfs, axis=0)\n",
    "# print(frame.info())\n",
    "# print(len(frame.groupby([' srcIp', ' dstIp', ' ipProto', ' srcPort', ' dstPort'])[' protocol'].count()))\n",
    "# print(frame.columns)\n",
    "\n",
    "# writing the concatanated df and reading the same again --> done to overcome the unknown error of duplicate rows not being dropped if this is not done\n",
    "frame.to_csv(outfile, index=False, na_rep='null', sep='|')\n",
    "# frame = pd.read_csv('DUP_CONCATANATED_AmazonEcho_44650d56ccd3_flowResult.csv')\n",
    "\n",
    "\n",
    "# # dropping the duplicate flows\n",
    "# result_frame = frame.drop_duplicates()\n",
    "# result_frame_1 = frame.drop_duplicates(keep=False)\n",
    "\n",
    "# # writing the resulting frame to a csv file\n",
    "# result_frame.to_csv('01DUP_CONCATANATED_AmazonEcho_44650d56ccd3_flowResult.csv', index=False, na_rep='null')\n",
    "# result_frame_1.to_csv('NODUP_AmazonEcho_44650d56ccd3_flowResult.csv.csv', index=False, na_rep='null')\n",
    "\n",
    "# print(\"lengths of before dropping duplicates:\", len(frame))\n",
    "# print(\"number of duplicate entries dropped:\", len(frame) - len(result_frame))\n",
    "# print(\"number of entries with duplicate entries:\", len(result_frame) - len(result_frame_1))\n",
    "# print(\"number of entries without duplicate entries:\", len(result_frame_1))\n",
    "# print(\"number of entries in result_frame:\", len(result_frame), end=\"\")\n",
    "# print(\"    (equal to the addition of the above 2)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ip_unique = result_frame.groupby([' srcIp', ' dstIp', ' ipProto', ' srcPort', ' dstPort']).count()  # [' protocol]'\n",
    "# print(df_ip_unique)\n",
    "# # df_1 = df.loc[df[' ipProto'].str.contains('6') & (df[' dstPort']=='8883')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
